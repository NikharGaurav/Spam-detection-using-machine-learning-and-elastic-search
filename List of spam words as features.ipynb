{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from elasticsearch import Elasticsearch \n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import elasticsearch_dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es=Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the file with mapping for each file with respect to being spam or ham\n",
    "spam_dict={}\n",
    "index_path='../full/trec07p/full/'\n",
    "for line in os.listdir(index_path):\n",
    "    file=open(index_path+line,\"r\", encoding=\"ISO-8859-1\")\n",
    "    for line in file:\n",
    "        line=line.strip().split(' ')\n",
    "        fname=line[1].split('/')\n",
    "        spam_dict[fname[2]]=line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#func to get a list of spam and ham files\n",
    "def get_spam_ham():\n",
    "    spams=[]\n",
    "    hams=[]\n",
    "    for file in spam_dict.keys():\n",
    "        if spam_dict[file]=='spam':\n",
    "            spams.append(file)\n",
    "        else:\n",
    "            hams.append(file)\n",
    "    return spams,hams\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_name='trec_spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating of list of possible spam terms\n",
    "features=list(set(['buy','miracle','virus','refinance','performance','prize','credit','million',\"hot\",\"naked\", \"fast\",\"viagra\",\"job\", \"lose\",\"win\",\"million\",\"limited\", \"sex\",\n",
    " 'diagnostics','profit','shopper','insurance','only','incredible','celebrity','meet','act','dollars','price','double','medication','here','chance','cialis','viagra','join','click','unsolicited','order',\n",
    " 'singles', '100','spam','percent','rate','ad','luxury','subscribe','freedom','bonus','free','fast','limited','hidden','now','earn','dollar',\n",
    " 'visit','medicine','weight','money','affordable','legal','trial','deal','valium','loans','lose','win','malware','discount','interest','instant','collect','no',\n",
    " 'cash','remove','claim','website','compare','check','success','love','sales','home','clearance','sex','c','lower','our','extra',\n",
    "\"enlargement\",\"100\",\"penis\",\"singles\",\"luxury\",\"erectile\",\"impotence\",\"erection\",\"hurry\",\"Cheapest\"]))\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating doc mapping and inverse doc mapping to ease of reading from file\n",
    "s = elasticsearch_dsl.Search(using=es, index=index_name, doc_type='document')\n",
    "#print(s)\n",
    "s = s.source([])  # only get ids, otherwise `fields` takes a list of field names\n",
    "docs = [h.meta.id for h in s.scan()]\n",
    "doc_map={doc:i+1 for i,doc in enumerate(docs)}\n",
    "inv_doc_map={i+1:doc for i,doc in enumerate(docs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opening file to store training and test set\n",
    "train_file=open('../Files/train_set.txt','a')\n",
    "test_file=open('../Files/test_set.txt','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating spam dictionary with each feature as key and ids as value\n",
    "def update_spamdict(dataset,spam_words):\n",
    "    spamdict={}\n",
    "    for word in spam_words:\n",
    "        body = {\n",
    "          \"query\": { \n",
    "            \"bool\": { \n",
    "               \"must\": [\n",
    "                   { \"match\": {\"split\":dataset}}, \n",
    "                       ],\n",
    "            \"filter\": [ \n",
    "                 { \"term\":  {\"body\":word }}\n",
    "              ]\n",
    "           }\n",
    "       },\n",
    "            \"size\": 75419\n",
    "        }\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        ids = [d['_id'] for d in res['hits']['hits']]\n",
    "        spamdict[word]=ids\n",
    "    return spamdict\n",
    "#created the training and testing matrix\n",
    "def create_matrix(dataset,doc_to_id,spam_words):\n",
    "  #  spamdict=update_spamdict(dataset,spam_words)\n",
    "    if dataset=='training'  :\n",
    "        spamdict=update_spamdict('training',spam_words)\n",
    "        ftrain=open(\"../Files/train_set.txt\",\"w\")\n",
    "        body={\n",
    "          \"query\": {\n",
    "              \"term\" : { \"split\" : \"training\" } \n",
    "                   },\n",
    "                  \"size\": 75419}\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        ids = [d['_id'] for d in res['hits']['hits']]\n",
    "        count=0\n",
    "        print(\"Started writing into file for training\")\n",
    "        for each_id in ids:\n",
    "            \n",
    "            temp_list=[]\n",
    "            for each_word in spam_words:\n",
    "                try:\n",
    "                    if each_id in spamdict[each_word]:\n",
    "                         temp_list.append(\"1\")\n",
    "                    else: \n",
    "                         temp_list.append(\"0\")\n",
    "                except:\n",
    "                    temp_list.append(\"0\")\n",
    "            label=es.get(index=index_name, doc_type='document', id=each_id)['_source']['label']\n",
    "            if label=='yes':\n",
    "                spameval=1\n",
    "            else:\n",
    "                spameval=0\n",
    "            binary_list=\" \".join(temp_list)\n",
    "            ftrain.write(str(doc_to_id[each_id]) + \" \"+str(spameval)+\" \"+binary_list+\"\\n\")\n",
    "            count+=1\n",
    "            if (count%1000==0):\n",
    "                print(\"printed:\"+str(count))\n",
    "        ftrain.close()\n",
    "        print(\"completed for training\")\n",
    "    else:\n",
    "        spamdict=update_spamdict('testing',spam_words)\n",
    "        ftest=open(\"../Files/test_set.txt\",\"w\")\n",
    "        body={\n",
    "          \"query\": {\n",
    "              \"term\" : { \"split\" : \"testing\" } \n",
    "                   },\n",
    "                  \"size\": 75419}\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        ids = [d['_id'] for d in res['hits']['hits']]\n",
    "        count=0\n",
    "        print(\"started writing for testing\")\n",
    "        for each_id in ids:\n",
    "            \n",
    "            temp_list=[]\n",
    "            for each_word in spam_words:\n",
    "                try:\n",
    "                    if each_id in spamdict[each_word]:\n",
    "                        temp_list.append(\"1\")\n",
    "                    else:\n",
    "                        temp_list.append(\"0\")\n",
    "                except:\n",
    "                        temp_list.append(\"0\")\n",
    "            label=es.get(index=index_name, doc_type='document', id=each_id)['_source']['label']\n",
    "            if label=='yes':\n",
    "                spameval=1\n",
    "            else:\n",
    "                spameval=0\n",
    "            binary_list=\" \".join(temp_list)\n",
    "            ftest.write(str(doc_to_id[each_id]) + \" \"+str(spameval)+\" \"+binary_list+\"\\n\")\n",
    "            count+=1\n",
    "            if count%1000==0:\n",
    "                print(\"printed:\"+str(count))\n",
    "        ftest.close()\n",
    "        print(\"completed for test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started writing into file for training\n",
      "printed:1000\n",
      "printed:2000\n",
      "printed:3000\n",
      "printed:4000\n",
      "printed:5000\n",
      "printed:6000\n",
      "printed:7000\n",
      "printed:8000\n",
      "printed:9000\n",
      "printed:10000\n",
      "printed:11000\n",
      "printed:12000\n",
      "printed:13000\n",
      "printed:14000\n",
      "printed:15000\n",
      "printed:16000\n",
      "printed:17000\n",
      "printed:18000\n",
      "printed:19000\n",
      "printed:20000\n",
      "printed:21000\n",
      "printed:22000\n",
      "printed:23000\n",
      "printed:24000\n",
      "printed:25000\n",
      "printed:26000\n",
      "printed:27000\n",
      "printed:28000\n",
      "printed:29000\n",
      "printed:30000\n",
      "printed:31000\n",
      "printed:32000\n",
      "printed:33000\n",
      "printed:34000\n",
      "printed:35000\n",
      "printed:36000\n",
      "printed:37000\n",
      "printed:38000\n",
      "printed:39000\n",
      "printed:40000\n",
      "printed:41000\n",
      "printed:42000\n",
      "printed:43000\n",
      "printed:44000\n",
      "printed:45000\n",
      "printed:46000\n",
      "printed:47000\n",
      "printed:48000\n",
      "printed:49000\n",
      "printed:50000\n",
      "printed:51000\n",
      "printed:52000\n",
      "printed:53000\n",
      "printed:54000\n",
      "printed:55000\n",
      "printed:56000\n",
      "printed:57000\n",
      "printed:58000\n",
      "printed:59000\n",
      "printed:60000\n",
      "completed for training\n",
      "started writing for testing\n",
      "printed:1000\n",
      "printed:2000\n",
      "printed:3000\n",
      "printed:4000\n",
      "printed:5000\n",
      "printed:6000\n",
      "printed:7000\n",
      "printed:8000\n",
      "printed:9000\n",
      "printed:10000\n",
      "printed:11000\n",
      "printed:12000\n",
      "printed:13000\n",
      "printed:14000\n",
      "printed:15000\n",
      "completed for test\n"
     ]
    }
   ],
   "source": [
    "create_matrix('training',doc_map,features)\n",
    "create_matrix('test',doc_map,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_file=numpy.loadtxt('C:/Users/Nikhar/Downloads/Side_work/spam/Files/train_set.txt', usecols=range(2,92))\n",
    "train_set_label=numpy.loadtxt('C:/Users/Nikhar/Downloads/Side_work/spam/Files/train_set.txt', usecols=(1))\n",
    "test_set_file=numpy.loadtxt('C:/Users/Nikhar/Downloads/Side_work/spam/Files/test_set.txt', usecols=range(2,92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_label=numpy.loadtxt('C:/Users/Nikhar/Downloads/Side_work/spam/Files/test_set.txt', usecols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "test_result = gnb.fit(train_set_file,train_set_label).predict(test_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5574118271015646"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy using gaussian naive bayes\n",
    "accuracy_score(test_result,test_set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "test_result = clf.fit(train_set_file,train_set_label).predict(test_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9669185892336251"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy using decision tree\n",
    "accuracy_score(test_result,test_set_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
